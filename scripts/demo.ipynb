{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A demo script to use TAC depth encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import json\n",
    "from PIL import Image\n",
    "import requests\n",
    "import natsort\n",
    "os.chdir(\"/root/TAC/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imports\n",
    "from config.default import get_config\n",
    "from common.registry import registry\n",
    "\n",
    "config = get_config(\"config/v2/v2_tac.yaml\")\n",
    "\n",
    "model_cls = registry.get_model(config.MODEL.name)\n",
    "model = model_cls.from_config(config)\n",
    "ckpt = torch.load(\"tac_model.pth\")\n",
    "model.load_state_dict(ckpt[\"state_dict\"], strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_DEPTH = 0.0\n",
    "MAX_DEPTH = 10.0\n",
    "DEPTH_SCALE = 1000\n",
    "\n",
    "depth_path = \"test.png\"\n",
    "depth = Image.open(depth_path)\n",
    "depth = np.array(depth).astype(\"float32\") / DEPTH_SCALE  # to meters\n",
    "depth = np.clip(depth, MIN_DEPTH, MAX_DEPTH) # clip to [MIN_DEPTH, MAX_DEPTH]\n",
    "depth = (depth - MIN_DEPTH) / (MAX_DEPTH - MIN_DEPTH) # normalize to [0,1]\n",
    "depth = np.expand_dims(depth, axis=2).repeat(3, axis=2) # extend to 3 channels\n",
    "depth = model.depth_processor(depth, do_rescale=False, return_tensors=\"pt\").pixel_values # preprocess (resize, normalize and to tensor)\n",
    "\n",
    "depth_embedding = F.normalize(model.embed_depth(depth)) # get embedding with FC. the feature locates in a unified space with RGB modality\n",
    "depth_embedding_nofc = model.embed_depth(depth, fc=False) # get embedding without FC. may be used for other downstream fine-tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seperate the depth encoder and use it alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_encoder = model.depth_transformer\n",
    "torch.save(depth_encoder.state_dict(), \"depth_encoder.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPImageProcessor, CLIPVisionModel, CLIPVisionConfig\n",
    "config = CLIPVisionConfig()\n",
    "depth_encoder = CLIPVisionModel(config=config)\n",
    "ckpt = torch.load(\"depth_encoder.pth\")\n",
    "depth_encoder.load_state_dict(ckpt)\n",
    "depth_processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_DEPTH = 0.0\n",
    "MAX_DEPTH = 10.0\n",
    "DEPTH_SCALE = 1000\n",
    "\n",
    "depth_path = \"test.png\"\n",
    "depth = Image.open(depth_path)\n",
    "depth = np.array(depth).astype(\"float32\") / DEPTH_SCALE  # to meters\n",
    "depth = np.clip(depth, MIN_DEPTH, MAX_DEPTH) # clip to [MIN_DEPTH, MAX_DEPTH]\n",
    "depth = (depth - MIN_DEPTH) / (MAX_DEPTH - MIN_DEPTH) # normalize to [0,1]\n",
    "depth = np.expand_dims(depth, axis=2).repeat(3, axis=2) # extend to 3 channels\n",
    "depth = depth_processor(depth, do_rescale=False, return_tensors=\"pt\").pixel_values # preprocess (resize, normalize and to tensor)\n",
    "\n",
    "outputs = depth_encoder(pixel_values=depth)\n",
    "outputs = outputs[\"last_hidden_state\"][:, 0, :] # get embedding without FC. may be used for other downstream fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (depth_embedding_nofc!=outputs).sum()==0 # check consistency"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Push to hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.depth_transformer.push_to_hub(\"TAC-ViT-base\", safe_serialization=True)\n",
    "model.depth_transformer.push_to_hub(\"TAC-ViT-base\", safe_serialization=False)\n",
    "model.depth_processor.push_to_hub(\"TAC-ViT-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "334fccb0cee34726ae0d4808bd49f0c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/350M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b9ae0e3951e4ce3bc72636a29e627a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/350M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/RavenK/TAC-ViT-base-rgb/commit/920f1ab784efef0f8308a5420aa83f50de0e752d', commit_message='Upload processor', commit_description='', oid='920f1ab784efef0f8308a5420aa83f50de0e752d', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.image_transformer.push_to_hub(\"TAC-ViT-base-rgb\", safe_serialization=True)\n",
    "model.image_transformer.push_to_hub(\"TAC-ViT-base-rgb\", safe_serialization=False)\n",
    "model.image_processor.push_to_hub(\"TAC-ViT-base-rgb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPImageProcessor, CLIPVisionModel, CLIPVisionConfig\n",
    "import numpy as np\n",
    "tac_depth_model = CLIPVisionModel.from_pretrained(\"RavenK/TAC-ViT-base\")\n",
    "tac_depth_processor = CLIPImageProcessor.from_pretrained(\"RavenK/TAC-ViT-base\")\n",
    "\n",
    "# Assuming test.png is a depth image with a scale factor 1000\n",
    "MIN_DEPTH = 0.0\n",
    "MAX_DEPTH = 10.0\n",
    "DEPTH_SCALE = 1000\n",
    "\n",
    "depth_path = \"test.png\"\n",
    "depth = Image.open(depth_path)\n",
    "depth = np.array(depth).astype(\"float32\") / DEPTH_SCALE  # to meters\n",
    "depth = np.clip(depth, MIN_DEPTH, MAX_DEPTH) # clip to [MIN_DEPTH, MAX_DEPTH]\n",
    "depth = (depth - MIN_DEPTH) / (MAX_DEPTH - MIN_DEPTH) # normalize to [0,1]\n",
    "depth = np.expand_dims(depth, axis=2).repeat(3, axis=2) # extend to 3 channels\n",
    "depth = tac_depth_processor(depth, do_rescale=False, return_tensors=\"pt\").pixel_values # preprocess (resize, normalize and to tensor)\n",
    "\n",
    "outputs = tac_depth_model(pixel_values=depth)\n",
    "outputs = outputs[\"last_hidden_state\"][:, 0, :] # get embedding without FC. may be used for other downstream fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (depth_embedding_nofc!=outputs).sum()==0 # check consistency"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
