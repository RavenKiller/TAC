{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A demo script to use TAC depth encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import json\n",
    "from PIL import Image\n",
    "import requests\n",
    "import natsort\n",
    "os.chdir(\"/root/TAC/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /openai/clip-vit-base-patch32/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fc9e2cc8340>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 24ca4743-727c-4de8-aa7a-f1418d0991e5)')' thrown while requesting HEAD https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/config.json\n"
     ]
    }
   ],
   "source": [
    "import imports\n",
    "from config.default import get_config\n",
    "from common.registry import registry\n",
    "\n",
    "config = get_config(\"config/v2/v2_tac.yaml\")\n",
    "\n",
    "model_cls = registry.get_model(config.MODEL.name)\n",
    "model = model_cls.from_config(config)\n",
    "ckpt = torch.load(\"data/best.pth\")\n",
    "model.load_state_dict(ckpt[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_DEPTH = 0.0\n",
    "MAX_DEPTH = 10.0\n",
    "DEPTH_SCALE = 1000\n",
    "\n",
    "depth_path = \"test.png\"\n",
    "depth = Image.open(depth_path)\n",
    "depth = np.array(depth).astype(\"float32\") / DEPTH_SCALE  # to meters\n",
    "depth = np.clip(depth, MIN_DEPTH, MAX_DEPTH) # clip to [MIN_DEPTH, MAX_DEPTH]\n",
    "depth = (depth - MIN_DEPTH) / (MAX_DEPTH - MIN_DEPTH) # normalize to [0,1]\n",
    "depth = np.expand_dims(depth, axis=2).repeat(3, axis=2) # extend to 3 channels\n",
    "depth = model.depth_processor(depth, do_rescale=False, return_tensors=\"pt\").pixel_values # preprocess (resize, normalize and to tensor)\n",
    "\n",
    "depth_embedding = F.normalize(model.embed_depth(depth)) # get embedding with FC. the feature locates in a unified space with RGB modality\n",
    "depth_embedding_nofc = model.embed_depth(depth, fc=False) # get embedding without FC. may be used for other downstream fine-tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seperate the depth encoder and use it alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_encoder = model.depth_transformer\n",
    "torch.save(depth_encoder.state_dict(), \"depth_encoder.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPImageProcessor, CLIPVisionModel, CLIPVisionConfig\n",
    "config = CLIPVisionConfig()\n",
    "depth_encoder = CLIPVisionModel(config=config)\n",
    "ckpt = torch.load(\"depth_encoder.pth\")\n",
    "depth_encoder.load_state_dict(ckpt)\n",
    "depth_processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_DEPTH = 0.0\n",
    "MAX_DEPTH = 10.0\n",
    "DEPTH_SCALE = 1000\n",
    "\n",
    "depth_path = \"test.png\"\n",
    "depth = Image.open(depth_path)\n",
    "depth = np.array(depth).astype(\"float32\") / DEPTH_SCALE  # to meters\n",
    "depth = np.clip(depth, MIN_DEPTH, MAX_DEPTH) # clip to [MIN_DEPTH, MAX_DEPTH]\n",
    "depth = (depth - MIN_DEPTH) / (MAX_DEPTH - MIN_DEPTH) # normalize to [0,1]\n",
    "depth = np.expand_dims(depth, axis=2).repeat(3, axis=2) # extend to 3 channels\n",
    "depth = depth_processor(depth, do_rescale=False, return_tensors=\"pt\").pixel_values # preprocess (resize, normalize and to tensor)\n",
    "\n",
    "outputs = depth_encoder(pixel_values=depth)\n",
    "outputs = outputs[\"last_hidden_state\"][:, 0, :] # get embedding without FC. may be used for other downstream fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (depth_embedding_nofc!=outputs).sum()==0 # check consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
